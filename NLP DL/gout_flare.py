# -*- coding: utf-8 -*-
"""gout_flare.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13vgl-Fd1TtumoVYq8De9ylx9sxWJabHZ
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

df = pd.read_csv(r"/content/GOUT-CC-2019-CORPUS-SYNTHETIC.tsv", delimiter='\t')

print(df)
predict = df['Predict'].value_counts()
print(predict)
consensus = df['Consensus'].value_counts()
print(consensus)

df.info()

# Gout Body Locations
#Skipping stemming locations, vocabulary is small
goutBodyLocation = ['ARM','HIP','SHOULDER','WRIST','ANKLE','KNEE','TOE','FOOT','FEET','LEG','ELBOW','FINGER','THUMB']
goutBodyLocationAbbreviations = ['LLE','LUE','RUE','RLE','LE','UE']

# Past Medical History
pmhx = ['PMH','PMHX','HX','PMX']

# Gout Keywords from Table 1. https://onlinelibrary.wiley.com/doi/full/10.1002/acr.22324
gout_keywords=['gout','podagra','tophaceous','tophi','tophus']

#Alternative Gout Related Keywords from Table 1. https://onlinelibrary.wiley.com/doi/full/10.1002/acr.22324
alt_keywords=['acute flare','acute inflammatory process','allopurinol','arthritis','attack','big toe','cellulitis',
'codeine','colchicine','chronic arthritis','corticosteroids', 'diclofenac','edema','elevated levels of uric acid',
'flare','flare up','flare‐up','g6pd    ','gonagra','high uric acid level','hydrocodone','hyperuricemia','ibuprofen'
,'indomethacin','inflammation of joint','joint pain','kidney stone','king\'s disease','metacarpal',
'metacarpophalangeal joint','metatarsal phalangeal','metatarsal‐phalangeal','naprosyn','naproxen','nsaid',
'oxycodone','recu    rrent attacks','red joint','redness and swelling','swelling','swollen joint','synovial biopsy',
'synovial fluid analysis','tender joint','urate lower     drugs','urate‐lowering therapy','urate nephropathy',
'uric acid','uric acid crystals','uric crystals','voltarol','zyloric'
]

import re
goutRegex = re.compile('.*gout.*',re.IGNORECASE)

def fetchGoutDictionaryWord(cc):
    if((re.search("|".join(gout_keywords), cc.lower())) !=None):
        findgoutkeyword = (re.search("|".join(gout_keywords), cc.lower()))
        if(findgoutkeyword!=None):
            #print(findgoutkeyword)
            return findgoutkeyword.group(0);
        return None

def fetchAltDictionaryWord(cc):
    if((re.search("|".join(alt_keywords), cc.lower())) !=None):
        findgoutkeyword = (re.search("|".join(alt_keywords), cc.lower()))
        if(findgoutkeyword!=None):
            #print(findgoutkeyword)
            return findgoutkeyword.group(0);
        return None

def hasGoutKeyword(cc):
    if((re.search("|".join(gout_keywords), cc.lower())) !=None):
        findgoutkeyword = (re.search("|".join(gout_keywords), cc.lower()))
        if(findgoutkeyword!=None):
            #print(findgoutkeyword)
            return True;
        return False

def hasAltGoutKeyword(cc):
    if((re.search("|".join(alt_keywords), cc.lower())) !=None):
        findgoutkeyword = (re.search("|".join(alt_keywords), cc.lower()))
        if(findgoutkeyword!=None):
            #print(findgoutkeyword)
            return True;
        return False

def hasGoutPmhx(cc):
    if(goutRegex.match(cc)!=None):
        findgout = (re.search('GOUT', cc.upper())).start()
        if((re.search("|".join(pmhx), cc.upper())) !=None):
            findpmhx = (re.search("|".join(pmhx), cc.upper())).start()
            if(findgout>findpmhx):
                return True
    return False

def hasGoutCurrent(cc):
    if(goutRegex.match(cc)!=None):
        findgout = (re.search('GOUT', cc.upper())).start()
        if((re.search("|".join(pmhx), cc.upper())) !=None):
            findpmhx = (re.search("|".join(pmhx), cc.upper())).start()
            if(findgout<findpmhx):
                return True
    return False


def hasGoutBodyLocationCurrent(cc):
    goutblmatch = re.search("|".join(goutBodyLocation), cc.upper())
    if(goutblmatch!=None):
        findgoutbl = goutblmatch.start()
        if((re.search("|".join(pmhx), cc.upper())) !=None):
            findpmhx = (re.search("|".join(pmhx), cc.upper())).start()
            if(findgoutbl<findpmhx):
                return True
    return False

# Test Code
assert(hasGoutCurrent('gout flare - pmh DM'))
assert(hasGoutCurrent('bar fight, multiple abrasions - pmhx gout, HT')==False)
assert(hasGoutPmhx('bar fight, multiple abrasions - pmhx gout, HT'))
assert(hasGoutBodyLocationCurrent('knee pain - pmh DM gout'))
#################################################
#Classifiers

# Replication of Stu's results
def regexGoutClassifier(cc):
    if(goutRegex.match(cc)!=None):
        return '__label__Y'
    else:
        return '__label__N'

def regexGoutCurrentClassifier(cc):
    if(hasGoutCurrent(cc)):
        return '__label__Y'
    return '__label__N'

def regexGoutKeywordClassifier(cc):
    if(hasGoutKeyword(cc)):
        return '__label__Y'
    return '__label__N'

def regexAltGoutKeywordClassifier(cc):
    if(hasAltGoutKeyword(cc)):
        return '__label__Y'
    return '__label__N'

def regexGoutBodyLocationOrCurrentGoutClassifier(cc):
    if(goutRegex.match(cc)!=None):
        if(hasGoutBodyLocationCurrent(cc)):
            return '__label__Y'
    if(hasGoutCurrent(cc)):
        return '__label__Y'
    return '__label__N'

def regexGoutBodyLocationAndAnyGoutClassifier(cc):
    if(goutRegex.match(cc)!=None):
        if(hasGoutBodyLocationCurrent(cc)):
            return '__label__Y'
    return '__label__N'

df.rename(columns={'Chief Complaint': 'CC'}, inplace=True)


pred = df['Predict'] == 'Y'
con = df['Consensus'] == 'N'
pd.options.display.max_colwidth = 110
disagree = pd.DataFrame(df[pred & con])['CC']
print(disagree)
len(disagree)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Read the data
df = pd.read_csv(r"/content/GOUT-CC-2019-CORPUS-SYNTHETIC.tsv", delimiter='\t')

# Display the data and value counts for Predict and Consensus columns
print(df)
predict = df['Predict'].value_counts()
print(predict)
consensus = df['Consensus'].value_counts()
print(consensus)

# Gout Body Locations
goutBodyLocation = ['ARM', 'HIP', 'SHOULDER', 'WRIST', 'ANKLE', 'KNEE', 'TOE', 'FOOT', 'FEET', 'LEG', 'ELBOW', 'FINGER', 'THUMB']
goutBodyLocationAbbreviations = ['LLE', 'LUE', 'RUE', 'RLE', 'LE', 'UE']

# Past Medical History
pmhx = ['PMH', 'PMHX', 'HX', 'PMX']

# Gout Keywords
gout_keywords = ['gout', 'podagra', 'tophaceous', 'tophi', 'tophus']
alt_keywords = [
    'acute flare', 'acute inflammatory process', 'allopurinol', 'arthritis', 'attack', 'big toe', 'cellulitis',
    'codeine', 'colchicine', 'chronic arthritis', 'corticosteroids', 'diclofenac', 'edema', 'elevated levels of uric acid',
    'flare', 'flare up', 'flare‐up', 'g6pd', 'gonagra', 'high uric acid level', 'hydrocodone', 'hyperuricemia', 'ibuprofen',
    'indomethacin', 'inflammation of joint', 'joint pain', 'kidney stone', 'king\'s disease', 'metacarpal',
    'metacarpophalangeal joint', 'metatarsal phalangeal', 'metatarsal‐phalangeal', 'naprosyn', 'naproxen', 'nsaid',
    'oxycodone', 'recurrent attacks', 'red joint', 'redness and swelling', 'swelling', 'swollen joint', 'synovial biopsy',
    'synovial fluid analysis', 'tender joint', 'urate lowering drugs', 'urate‐lowering therapy', 'urate nephropathy',
    'uric acid', 'uric acid crystals', 'uric crystals', 'voltarol', 'zyloric'
]

# Define the body2counts function
def body2counts(thelist, df, thecase):
    bodydf = pd.DataFrame(df['CC'].copy())
    ccseries = pd.Series(bodydf["CC"])
    counts = {}
    for item in thelist:
        bodydf[item] = ccseries.str.contains(item, regex=True, case=thecase)
    for item in thelist:
        countdf = bodydf.loc[bodydf[item] == True]
        counts[item] = len(countdf)
    return counts

# Rename column for convenience
df.rename(columns={'Chief Complaint': 'CC'}, inplace=True)

# Get the counts for body locations and PMH
body_count = body2counts(goutBodyLocation, df, False)
bodybits = pd.Series(body_count)
pmh_count = body2counts(pmhx, df, False)
pmhbits = pd.Series(pmh_count)

# Plot the distribution of body locations
plt.figure(figsize=(12, 6))
sns.barplot(x=bodybits.index, y=bodybits.values, alpha=0.8)
plt.ylabel('Occurrences', fontsize=21)
plt.xlabel('Normalized Cue', fontsize=21)
plt.show()

# Plot the distribution of PMH
plt.figure(figsize=(12, 6))
sns.barplot(x=pmhbits.index, y=pmhbits.values, alpha=0.8)
plt.ylabel('Occurrences', fontsize=21)
plt.xlabel('PMH Terms', fontsize=21)
plt.show()

# Print value counts for Predict and Consensus columns
print("Predict value counts:")
print(predict)
print("\nConsensus value counts:")
print(consensus)

# Display rows where Predict is 'Y' and Consensus is 'N'
pred = df['Predict'] == 'Y'
con = df['Consensus'] == 'N'
pd.options.display.max_colwidth = 110
disagree = pd.DataFrame(df[pred & con])['CC']
print(disagree)
print("Number of disagreements:", len(disagree))

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Read the data
df = pd.read_csv(r"/content/GOUT-CC-2019-CORPUS-SYNTHETIC.tsv", delimiter='\t')

# Gout Body Locations
goutBodyLocation = ['ARM', 'HIP', 'SHOULDER', 'WRIST', 'ANKLE', 'KNEE', 'TOE', 'FOOT', 'FEET', 'LEG', 'ELBOW', 'FINGER', 'THUMB']

# Past Medical History
pmhx = ['PMH', 'PMHX', 'HX', 'PMX']

# Define the body2counts function
def body2counts(thelist, df, thecase):
    bodydf = pd.DataFrame(df['CC'].copy())
    ccseries = pd.Series(bodydf["CC"])
    counts = {}
    for item in thelist:
        bodydf[item] = ccseries.str.contains(item, regex=True, case=thecase)
    for item in thelist:
        countdf = bodydf.loc[bodydf[item] == True]
        counts[item] = len(countdf)
    return counts

# Rename column for convenience
df.rename(columns={'Chief Complaint': 'CC'}, inplace=True)

# Plot distribution of body locations in all people
body_count = body2counts(goutBodyLocation, df, False)
bodybits = pd.Series(body_count)

# Plot distribution of PMH
pmh_count = body2counts(pmhx, df, False)
pmhbits = pd.Series(pmh_count)

plt.figure(figsize=(12, 6))
sns.barplot(x=bodybits.index, y=bodybits.values, alpha=0.8)
plt.ylabel('Occurrences', fontsize=21)
plt.xlabel('Normalized Cue', fontsize=21)
plt.show()

# Show Gout vs. Not Gout
goutbitdf = df.loc[df["Consensus"] == 'Y']
gout_body_count = body2counts(goutBodyLocation, goutbitdf, False)
goutbodybits = pd.Series(gout_body_count)

plt.figure(figsize=(12, 6))
sns.barplot(x=goutbodybits.index, y=goutbodybits.values, alpha=0.8)
plt.ylabel('Occurrences', fontsize=21)
plt.xlabel('Normalized Cue', fontsize=21)
plt.show()

notgoutbitdf = df.loc[df["Consensus"] == 'N']
notgout_body_count = body2counts(goutBodyLocation, notgoutbitdf, False)
notgoutbodybits = pd.Series(notgout_body_count)

index = [*bodybits.keys()]
gout = [*gout_body_count.values()]
notgout = [*notgout_body_count.values()]

goutfolks = sum(gout)
notgoutfolks = sum(notgout)

print(goutfolks)
print(notgoutfolks)

goutpercent = [x / goutfolks for x in gout]
notgoutpercent = [x / notgoutfolks for x in notgout]

goutDifdf = pd.DataFrame({'Gout Flare': goutpercent, 'No Gout Flare': notgoutpercent}, index=index)
goutDifdf.plot.bar(rot=0, figsize=(11, 5.5))

# PMH Gout Chart
gout_pmh_count = body2counts(pmhx, goutbitdf, False)
goutpmhbits = pd.Series(gout_pmh_count)
notgout_pmh_count = body2counts(pmhx, notgoutbitdf, False)
notgoutpmhbits = pd.Series(notgout_pmh_count)

index = [*pmhbits.keys()]
gout = [*gout_pmh_count.values()]
notgout = [*notgout_pmh_count.values()]

goutfolks = sum(gout)
notgoutfolks = sum(notgout)

goutpercent = [x / goutfolks for x in gout]
notgoutpercent = [x / notgoutfolks for x in notgout]

plt.figure(figsize=(12, 6))
sns.barplot(x=index, y=goutpercent, alpha=0.8, label='Gout Flare')
sns.barplot(x=index, y=notgoutpercent, alpha=0.8, label='No Gout Flare', color='r')
plt.legend()
plt.ylabel('Percentage', fontsize=21)
plt.xlabel('PMH Terms', fontsize=21)
plt.show()

# Abbreviations
abbr_body_count = body2counts(goutBodyLocationAbbreviations, df, True)
abbrbodybits = pd.Series(abbr_body_count)

plt.figure(figsize=(12, 6))
sns.barplot(x=abbrbodybits.index, y=abbrbodybits.values, alpha=0.8)
plt.ylabel('Occurrences', fontsize=21)
plt.xlabel('Abbreviations', fontsize=21)

## Predicting Flare Gout Status on Chief Complaint with NLP
### Divide Chart Review Consensus data into validation, train and test sets
from sklearn.model_selection import train_test_split
train, test = train_test_split(df, test_size=0.2)
pred_labels=['__label__Y','__label__N','__label__U']


#Format data
data = df[['Consensus', 'CC']].rename(columns={"Consensus":"label", "CC":"text"})
pd.options.display.max_colwidth = 60
data['label'] = '__label__' + data['label'].astype(str)
print(data[1:10])

data.iloc[0:int(len(data)*0.8)].to_csv(r"/content/train.tsv", sep='\t', index = False, header = False)
data.iloc[int(len(data)*0.8):int(len(data)*0.9)].to_csv(r"/content/test.tsv", sep='\t', index = False, header = False)
data.iloc[int(len(data)*0.9):].to_csv(r"/content/'dev.tsv", sep='\t', index = False, header = False);

# Test Regular Expression Classifiers
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

def showConfusionMatrix(heading,y_true,y_pred):
    regex1data  = {'y_Actual': y_true,'y_Predicted': y_pred}
    df = pd.DataFrame(regex1data, columns=['y_Actual','y_Predicted'])
    confusion_matrix = pd.crosstab(df['y_Actual'], df['y_Predicted'], rownames=['Actual'], colnames=['Predicted'],)
    ax = plt.axes()
    ax = sns.heatmap(confusion_matrix, annot=True,fmt="d",ax=ax)
    ax.set_title(heading)
    plt.show()
    print(classification_report(y_true, y_pred, target_names=pred_labels, labels=pred_labels))


y_true = data['label']
#Current Gout Classifier
y_pred = (pd.DataFrame(data['text']).applymap(regexGoutClassifier))['text']
title2display='Gout (no PMHx) Classifier'
showConfusionMatrix(title2display,y_true,y_pred)

#Current Gout Only Classifier
y_pred = (pd.DataFrame(data['text']).applymap(regexGoutCurrentClassifier))['text']
title2display='Current Gout Classifier'
showConfusionMatrix(title2display,y_true,y_pred)

#Current Gout Or Current Body Location Classifier
y_pred = (pd.DataFrame(data['text']).applymap(regexGoutBodyLocationOrCurrentGoutClassifier))['text']
title2display='Current Gout OR (Gout Body Location And Any Gout) Classifier'
showConfusionMatrix(title2display,y_true,y_pred)

#Gout Or Current Body Location Classifier
y_pred = (pd.DataFrame(data['text']).applymap(regexGoutBodyLocationAndAnyGoutClassifier))['text']
title2display='Gout Anywhere AND Body Location Classifier'
showConfusionMatrix(title2display,y_true,y_pred)

#Format data
data = df[['Predict', 'CC']].rename(columns={"Predict":"label", "CC":"text"})
pd.options.display.max_colwidth = 60
data['label'] = '__label__' + data['label'].astype(str)
print(data[1:10])

data.iloc[0:int(len(data)*0.8)].to_csv('train.tsv', sep='\t', index = False, header = False)
data.iloc[int(len(data)*0.8):int(len(data)*0.9)].to_csv('test.tsv', sep='\t', index = False, header = False)
data.iloc[int(len(data)*0.9):].to_csv('dev.tsv', sep='\t', index = False, header = False);


y_true = data['label']
#Current Gout Classifier
y_pred = (pd.DataFrame(data['text']).applymap(regexGoutClassifier))['text']
title2display='Gout (no PMHx) Classifier'
showConfusionMatrix(title2display,y_true,y_pred)

#Current Gout Only Classifier
y_pred = (pd.DataFrame(data['text']).applymap(regexGoutCurrentClassifier))['text']
title2display='Current Gout Classifier'
showConfusionMatrix(title2display,y_true,y_pred)

#Current Gout Or Current Body Location Classifier
y_pred = (pd.DataFrame(data['text']).applymap(regexGoutBodyLocationOrCurrentGoutClassifier))['text']
title2display='Current Gout OR (Gout Body Location And Any Gout) Classifier'
showConfusionMatrix(title2display,y_true,y_pred)

#Gout Or Current Body Location Classifier
y_pred = (pd.DataFrame(data['text']).applymap(regexGoutBodyLocationAndAnyGoutClassifier))['text']
title2display='Gout Anywhere AND Body Location Classifier'
showConfusionMatrix(title2display,y_true,y_pred)

## BERT Approach
#### Boilerplate GPU/CUDA Validation
import torch
import sys
from subprocess import call, check_output, CalledProcessError

print('__Python VERSION:', sys.version)
print('__pyTorch VERSION:', torch.__version__)

# Check for CUDA availability
print('__CUDA AVAILABLE:', torch.cuda.is_available())

if torch.cuda.is_available():
    print('__CUDA VERSION')
    try:
        nvcc_output = check_output(["nvcc", "--version"]).decode()
        print(nvcc_output)
    except FileNotFoundError:
        print('nvcc is not found. Make sure CUDA is installed correctly.')

    print('__CUDNN VERSION:', torch.backends.cudnn.version())
    print('__Number CUDA Devices:', torch.cuda.device_count())

    print('__Devices')
    try:
        nvidia_smi_output = check_output(["nvidia-smi", "--format=csv", "--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free"]).decode()
        print(nvidia_smi_output)
    except FileNotFoundError:
        print('nvidia-smi is not found. Make sure NVIDIA drivers are installed correctly.')

    print('Active CUDA Device: GPU', torch.cuda.current_device())
    print('Available devices ', torch.cuda.device_count())
    print('Current cuda device ', torch.cuda.current_device())
else:
    print('CUDA is not available. Please install CUDA and the appropriate drivers.')

# Alternatively, display CUDA device properties using PyTorch
if torch.cuda.is_available():
    for i in range(torch.cuda.device_count()):
        print(f"Device {i}: {torch.cuda.get_device_name(i)}")
        print(f"Memory Usage: {torch.cuda.memory_allocated(i)/1024**3:.2f} GB allocated, {torch.cuda.memory_reserved(i)/1024**3:.2f} GB reserved")

pip install flair

!pip install flair

import pandas as pd
from flair.data import Corpus
from flair.datasets import ClassificationCorpus
from flair.embeddings import TransformerDocumentEmbeddings
from flair.models import TextClassifier
from flair.trainers import ModelTrainer
from sklearn.model_selection import train_test_split
from pathlib import Path

# 1. Load and inspect the data
df = pd.read_csv(r"/content/GOUT-CC-2019-CORPUS-SYNTHETIC.tsv", delimiter='\t')

# Print the first few rows of the DataFrame to verify column names
print(df.head())
print(df.columns)

# Check if 'Chief Complaint' and 'Consensus' columns exist in the DataFrame
if 'Chief Complaint' not in df.columns or 'Consensus' not in df.columns:
    raise KeyError("The required columns 'Chief Complaint' and/or 'Consensus' are not present in the DataFrame")

# 2. Split the data into train and test sets
train, test = train_test_split(df, test_size=0.2)

# 3. Format the data for flair
data = df[['Consensus', 'Chief Complaint']].rename(columns={"Consensus":"label", "Chief Complaint":"text"})
data['label'] = '__label__' + data['label'].astype(str)

# Save the data in the required format
data.iloc[0:int(len(data)*0.8)].to_csv('train.tsv', sep='\t', index=False, header=False)
data.iloc[int(len(data)*0.8):int(len(data)*0.9)].to_csv('test.tsv', sep='\t', index=False, header=False)
data.iloc[int(len(data)*0.9):].to_csv('dev.tsv', sep='\t', index=False, header=False)

# 4. Create corpus
corpus = ClassificationCorpus(Path('./'), test_file='test.tsv', dev_file='dev.tsv', train_file='train.tsv')

# 5. Train with Transformer Embeddings (BERT)
document_embeddings = TransformerDocumentEmbeddings('bert-base-uncased')

classifier = TextClassifier(document_embeddings, label_dictionary=corpus.make_label_dictionary())

trainer = ModelTrainer(classifier, corpus)
trainer.train(
    './',
    learning_rate=0.1,
    mini_batch_size=10,
    anneal_factor=0.5,
    patience=5,
    max_epochs=4
)

# 6. Plot weight traces
from flair.visual.training_curves import Plotter
plotter = Plotter()
plotter.plot_weights('./loss.tsv')

import pandas as pd
from flair.data import Corpus
from flair.datasets import ClassificationCorpus
from flair.embeddings import TransformerDocumentEmbeddings
from flair.models import TextClassifier
from flair.trainers import ModelTrainer
from sklearn.model_selection import train_test_split
from pathlib import Path

# 1. Load and inspect the data
df = pd.read_csv(r"/content/GOUT-CC-2019-CORPUS-SYNTHETIC.tsv", delimiter='\t')

# Print the first few rows of the DataFrame to verify column names
print(df.head())
print(df.columns)

# Check if 'Chief Complaint' and 'Consensus' columns exist in the DataFrame
if 'Chief Complaint' not in df.columns or 'Consensus' not in df.columns:
    raise KeyError("The required columns 'Chief Complaint' and/or 'Consensus' are not present in the DataFrame")

# 2. Split the data into train and test sets
train, test = train_test_split(df, test_size=0.2)

# 3. Format the data for flair
data = df[['Consensus', 'Chief Complaint']].rename(columns={"Consensus":"label", "Chief Complaint":"text"})
data['label'] = '__label__' + data['label'].astype(str)

# Save the data in the required format
data.iloc[0:int(len(data)*0.8)].to_csv(r'/content/train.tsv', sep='\t', index=False, header=False)
data.iloc[int(len(data)*0.8):int(len(data)*0.9)].to_csv(r'/content/test.tsv', sep='\t', index=False, header=False)
data.iloc[int(len(data)*0.9):].to_csv(r'/content/dev.tsv', sep='\t', index=False, header=False)

# 4. Create corpus
corpus = ClassificationCorpus(Path('./'), test_file=r'/content/test.tsv', dev_file=r'/content/dev.tsv', train_file=r'/content/train.tsv')

# 5. Train with Transformer Embeddings (BERT)
document_embeddings = TransformerDocumentEmbeddings('bert-base-uncased')

# Specify the label type (default is 'class')
label_type = 'class'
label_dictionary = corpus.make_label_dictionary(label_type=label_type)

classifier = TextClassifier(document_embeddings, label_dictionary=label_dictionary, label_type=label_type)

trainer = ModelTrainer(classifier, corpus)
trainer.train(
    './',
    learning_rate=0.1,
    mini_batch_size=10,
    anneal_factor=0.5,
    patience=5,
    max_epochs=4
)

# 6. Plot weight traces
from flair.visual.training_curves import Plotter
plotter = Plotter()
plotter.plot_weights(r'/content/loss.tsv')

import pandas as pd
from flair.data import Corpus
from flair.datasets import ClassificationCorpus
from flair.embeddings import TransformerDocumentEmbeddings
from flair.models import TextClassifier
from flair.trainers import ModelTrainer
from sklearn.model_selection import train_test_split
from pathlib import Path

# 1. Load and inspect the data
df = pd.read_csv(r"/content/GOUT-CC-2019-CORPUS-SYNTHETIC.tsv", delimiter='\t')

# Print the first few rows of the DataFrame to verify column names
print(df.head())
print(df.columns)

# Check if 'Chief Complaint' and 'Consensus' columns exist in the DataFrame
if 'Chief Complaint' not in df.columns or 'Consensus' not in df.columns:
    raise KeyError("The required columns 'Chief Complaint' and/or 'Consensus' are not present in the DataFrame")

# 2. Split the data into train and test sets
train, test = train_test_split(df, test_size=0.2)

# 3. Format the data for flair
data = df[['Consensus', 'Chief Complaint']].rename(columns={"Consensus":"label", "Chief Complaint":"text"})
data['label'] = '__label__' + data['label'].astype(str)

# Save the data in the required format
data.iloc[0:int(len(data)*0.8)].to_csv(r'/content/train.tsv', sep='\t', index=False, header=False)
data.iloc[int(len(data)*0.8):int(len(data)*0.9)].to_csv(r'/content/test.tsv', sep='\t', index=False, header=False)
data.iloc[int(len(data)*0.9):].to_csv(r'/content/dev.tsv', sep='\t', index=False, header=False)

# 4. Create corpus
corpus = ClassificationCorpus(Path('./'), test_file=r'/content/test.tsv', dev_file=r'/content/dev.tsv', train_file=r'/content/train.tsv')

# 5. Train with Transformer Embeddings (BERT)
document_embeddings = TransformerDocumentEmbeddings('bert-base-uncased')

# Specify the label type (default is 'class')
label_type = 'class'
label_dictionary = corpus.make_label_dictionary(label_type=label_type)

classifier = TextClassifier(document_embeddings, label_dictionary=label_dictionary, label_type=label_type)

trainer = ModelTrainer(classifier, corpus)
trainer.train(
    './',
    learning_rate=0.1,
    mini_batch_size=10,
    anneal_factor=0.5,
    patience=5,
    max_epochs=4
)

# 6. Plot weight traces
from flair.visual.training_curves import Plotter
plotter = Plotter()
plotter.plot_training_curves('./loss.tsv')

import pandas as pd
from flair.data import Corpus
from flair.datasets import ClassificationCorpus
from flair.embeddings import TransformerDocumentEmbeddings
from flair.models import TextClassifier
from flair.trainers import ModelTrainer
from sklearn.model_selection import train_test_split
from pathlib import Path

# 1. Load and inspect the data
df = pd.read_csv(r"/content/GOUT-CC-2019-CORPUS-SYNTHETIC.tsv", delimiter='\t')

# Print the first few rows of the DataFrame to verify column names
print(df.head())
print(df.columns)

# Check if 'Chief Complaint' and 'Consensus' columns exist in the DataFrame
if 'Chief Complaint' not in df.columns or 'Consensus' not in df.columns:
    raise KeyError("The required columns 'Chief Complaint' and/or 'Consensus' are not present in the DataFrame")

# 2. Split the data into train and test sets
train, test = train_test_split(df, test_size=0.2)

# 3. Format the data for flair
data = df[['Consensus', 'Chief Complaint']].rename(columns={"Consensus":"label", "Chief Complaint":"text"})
data['label'] = '__label__' + data['label'].astype(str)

# Save the data in the required format
data.iloc[0:int(len(data)*0.8)].to_csv(r'/content/train.tsv', sep='\t', index=False, header=False)
data.iloc[int(len(data)*0.8):int(len(data)*0.9)].to_csv(r'/content/test.tsv', sep='\t', index=False, header=False)
data.iloc[int(len(data)*0.9):].to_csv(r'/content/dev.tsv', sep='\t', index=False, header=False)

# 4. Create corpus
corpus = ClassificationCorpus(Path('./'), test_file=r'/content/test.tsv', dev_file=r'/content/dev.tsv', train_file=r'/content/train.tsv')

# 5. Train with Transformer Embeddings (BERT)
document_embeddings = TransformerDocumentEmbeddings('bert-base-uncased')

# Specify the label type (default is 'class')
label_type = 'class'
label_dictionary = corpus.make_label_dictionary(label_type=label_type)

classifier = TextClassifier(document_embeddings, label_dictionary=label_dictionary, label_type=label_type)

trainer = ModelTrainer(classifier, corpus)
trainer.train(
    base_path='./',
    learning_rate=0.1,
    mini_batch_size=10,
    anneal_factor=0.5,
    patience=5,
    max_epochs=4,
    embeddings_storage_mode='cpu'
)

# 6. Plot weight traces
from flair.visual.training_curves import Plotter
plotter = Plotter()
plotter.plot_training_curves(r/content/loss.tsv)

with open('/content/loss.tsv', 'r') as file:
    lines = file.readlines()
    for line in lines[:10]:  # Print first 10 lines
        print(line)

from flair.data import Corpus
from flair.datasets import ClassificationCorpus
from flair.embeddings import TransformerDocumentEmbeddings
from flair.models import TextClassifier
from flair.trainers import ModelTrainer
from sklearn.model_selection import train_test_split
from pathlib import Path

# 1. Load and inspect the data
df = pd.read_csv(r"/content/GOUT-CC-2019-CORPUS-SYNTHETIC.tsv", delimiter='\t')

# Check if 'Chief Complaint' and 'Consensus' columns exist in the DataFrame
if 'Chief Complaint' not in df.columns or 'Consensus' not in df.columns:
    raise KeyError("The required columns 'Chief Complaint' and/or 'Consensus' are not present in the DataFrame")

# 2. Split the data into train and test sets
train, test = train_test_split(df, test_size=0.2)

# 3. Format the data for flair
data = df[['Consensus', 'Chief Complaint']].rename(columns={"Consensus":"label", "Chief Complaint":"text"})
data['label'] = '__label__' + data['label'].astype(str)

# Save the data in the required format
data.iloc[0:int(len(data)*0.8)].to_csv(r'/content/train.tsv', sep='\t', index=False, header=False)
data.iloc[int(len(data)*0.8):int(len(data)*0.9)].to_csv(r'/content/test.tsv', sep='\t', index=False, header=False)
data.iloc[int(len(data)*0.9):].to_csv(r'/content/dev.tsv', sep='\t', index=False, header=False)

# 4. Create corpus
corpus = ClassificationCorpus(Path('./'), test_file=r'/content/test.tsv', dev_file=r'/content/dev.tsv', train_file=r'/content/train.tsv')

# 5. Train with Transformer Embeddings (BERT)
document_embeddings = TransformerDocumentEmbeddings('bert-base-uncased')
label_dictionary = corpus.make_label_dictionary(label_type='class')

classifier = TextClassifier(document_embeddings, label_dictionary=label_dictionary, label_type='class')

trainer = ModelTrainer(classifier, corpus)
trainer.train(
    base_path='./',
    learning_rate=0.1,
    mini_batch_size=10,
    anneal_factor=0.5,
    patience=5,
    max_epochs=4,
    embeddings_storage_mode='cpu'
)

# 6. Plot training curves
from flair.visual.training_curves import Plotter
plotter = Plotter()
plotter.plot_training_curves('./loss.tsv')

# Evaluate the Model
# Load and evaluate the best model
best_model = TextClassifier.load(r'/content/best-model.pt')
test_sentences = [Sentence(text) for text in test['Chief Complaint']]
best_model.predict(test_sentences)
y_pred_best = [sentence.labels[0].value for sentence in test_sentences]
y_true = test['Consensus'].values
print("Best Model Evaluation:")
print(classification_report(y_true, y_pred_best))

# Load and evaluate the final model
final_model = TextClassifier.load(r'/content/final-model.pt')
final_model.predict(test_sentences)
y_pred_final = [sentence.labels[0].value for sentence in test_sentences]
print("Final Model Evaluation:")
print(classification_report(y_true, y_pred_final))

pip install tk

!pip install torch transformers scikit-learn

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Load the dataset
df = pd.read_csv(r"/content/GOUT-CC-2019-CORPUS-SYNTHETIC.tsv", delimiter='\t')

# Encode labels
label_encoder = LabelEncoder()
df['Consensus'] = label_encoder.fit_transform(df['Consensus'])

# Split the data
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# Load RoBERTa model and tokenizer
tokenizer_roberta = AutoTokenizer.from_pretrained("allenai/biomed_roberta_base")
model_roberta = AutoModelForSequenceClassification.from_pretrained("allenai/biomed_roberta_base")

# Define a function to predict labels using RoBERTa
def predict_roberta(text):
    inputs = tokenizer_roberta(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    with torch.no_grad():
        outputs = model_roberta(**inputs)
    logits = outputs.logits
    predicted_class_id = torch.argmax(logits, dim=1).item()
    return label_encoder.inverse_transform([predicted_class_id])[0], torch.softmax(logits, dim=1).max().item()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
import torch
from sklearn.metrics import classification_report

# Load the dataset
df = pd.read_csv(r"/content/GOUT-CC-2019-CORPUS-SYNTHETIC.tsv", delimiter='\t')

# Encode labels
label_encoder = LabelEncoder()
df['Consensus'] = label_encoder.fit_transform(df['Consensus'])

# Split the data
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# Load RoBERTa model and tokenizer
tokenizer = AutoTokenizer.from_pretrained("allenai/biomed_roberta_base")
model = AutoModelForSequenceClassification.from_pretrained("allenai/biomed_roberta_base", num_labels=len(label_encoder.classes_))

# Tokenize the data
def tokenize_function(examples):
    return tokenizer(examples["Chief Complaint"], padding="max_length", truncation=True)

train_encodings = tokenizer(list(train_df['Chief Complaint']), truncation=True, padding=True, max_length=512)
test_encodings = tokenizer(list(test_df['Chief Complaint']), truncation=True, padding=True, max_length=512)

class GoutDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = GoutDataset(train_encodings, list(train_df['Consensus']))
test_dataset = GoutDataset(test_encodings, list(test_df['Consensus']))

# Define training arguments
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    evaluation_strategy="epoch"
)

# Define Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset
)

# Fine-tune the model
trainer.train()

# Evaluate the model
predictions = trainer.predict(test_dataset)
y_pred = predictions.predictions.argmax(axis=1)
y_true = test_df['Consensus'].values

print("RoBERTa Model Evaluation:")
print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
import torch
from sklearn.metrics import classification_report

# Load the dataset
df = pd.read_csv(r"/content/GOUT-CC-2019-CORPUS-SYNTHETIC.tsv", delimiter='\t')

# Encode labels
label_encoder = LabelEncoder()
df['Consensus'] = label_encoder.fit_transform(df['Consensus'])

# Split the data
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# Load RoBERTa model and tokenizer
tokenizer = AutoTokenizer.from_pretrained("allenai/biomed_roberta_base")
model = AutoModelForSequenceClassification.from_pretrained("allenai/biomed_roberta_base", num_labels=len(label_encoder.classes_))

# Tokenize the data
def tokenize_function(examples):
    return tokenizer(examples["Chief Complaint"], padding="max_length", truncation=True)

train_encodings = tokenizer(list(train_df['Chief Complaint']), truncation=True, padding=True, max_length=512)
test_encodings = tokenizer(list(test_df['Chief Complaint']), truncation=True, padding=True, max_length=512)

class GoutDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = GoutDataset(train_encodings, list(train_df['Consensus']))
test_dataset = GoutDataset(test_encodings, list(test_df['Consensus']))

# Define training arguments
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    evaluation_strategy="epoch"
)

# Define Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset
)

# Fine-tune the model
trainer.train()

# Evaluate the model
predictions = trainer.predict(test_dataset)
y_pred = predictions.predictions.argmax(axis=1)
y_true = test_df['Consensus'].values

# Get the unique labels in y_true
unique_labels = sorted(set(y_true))

# Filter the target names to match the unique labels
target_names = [label_encoder.classes_[i] for i in unique_labels]

print("RoBERTa Model Evaluation:")
print(classification_report(y_true, y_pred, target_names=target_names))

pip install --upgrade torch transformers

df.columns

import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# Load and preprocess data
df = pd.read_csv(r"/content/GOUT-CC-2019-CORPUS-SYNTHETIC.tsv", delimiter='\t')

# Convert labels to numerical values
df['label'] = df['Predict'].astype('category').cat.codes

# Check unique labels
num_classes = df['label'].nunique()
print(f"Number of classes: {num_classes}")

# Split data
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# Define a custom dataset class
class TextDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_len):
        self.dataframe = dataframe
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, idx):
        row = self.dataframe.iloc[idx]
        text = row['Chief Complaint']
        label = row['label']

        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            return_tensors='pt',
            padding='max_length',
            truncation=True
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# Initialize tokenizer and model
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

# Initialize model with the correct number of labels
model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_classes)

# Create DataLoader
train_dataset = TextDataset(train_df, tokenizer, max_len=128)
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)

test_dataset = TextDataset(test_df, tokenizer, max_len=128)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

# Set up optimizer
optimizer = AdamW(model.parameters(), lr=1e-5)

# Define training function
def train_model(model, data_loader, optimizer, device):
    model.train()
    total_loss = 0

    for batch in data_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    return total_loss / len(data_loader)

# Define evaluation function
def evaluate_model(model, data_loader, device):
    model.eval()
    y_true = []
    y_pred = []

    with torch.no_grad():
        for batch in data_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            preds = torch.argmax(outputs.logits, dim=-1)

            y_true.extend(labels.cpu().numpy())
            y_pred.extend(preds.cpu().numpy())

    return y_true, y_pred

# Training and evaluation
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

for epoch in range(4):
    train_loss = train_model(model, train_loader, optimizer, device)
    print(f"Epoch {epoch + 1}/{4}, Training Loss: {train_loss}")

# Evaluate the model
y_true, y_pred = evaluate_model(model, test_loader, device)

# Print classification report
print("Model Evaluation:")
print(classification_report(y_true, y_pred))

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW

# Load and preprocess data
df = pd.read_csv(r"/content/GOUT-CC-2019-CORPUS-SYNTHETIC.tsv", delimiter='\t')

# Convert labels to numerical values
df['label'] = df['Predict'].astype('category').cat.codes

# Check unique labels
num_classes = df['label'].nunique()
print(f"Number of classes: {num_classes}")

# Split data
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# Initialize tokenizer and model for DistilBERT
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_classes)

# Define a custom dataset class
class TextDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_len):
        self.dataframe = dataframe
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, idx):
        row = self.dataframe.iloc[idx]
        text = row['Chief Complaint']
        label = row['label']

        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            return_tensors='pt',
            padding='max_length',
            truncation=True
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# Create DataLoader for DistilBERT
train_dataset = TextDataset(train_df, tokenizer, max_len=128)
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)

test_dataset = TextDataset(test_df, tokenizer, max_len=128)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

# Set up optimizer for DistilBERT
optimizer = AdamW(model.parameters(), lr=1e-5)

# Define training function for DistilBERT
def train_model(model, data_loader, optimizer, device):
    model.train()
    total_loss = 0

    for batch in data_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    return total_loss / len(data_loader)

# Define evaluation function for DistilBERT
def evaluate_model(model, data_loader, device):
    model.eval()
    y_true = []
    y_pred = []

    with torch.no_grad():
        for batch in data_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            preds = torch.argmax(outputs.logits, dim=-1)

            y_true.extend(labels.cpu().numpy())
            y_pred.extend(preds.cpu().numpy())

    return y_true, y_pred

# Training and evaluation for DistilBERT
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

for epoch in range(4):
    train_loss = train_model(model, train_loader, optimizer, device)
    print(f"Epoch {epoch + 1}/4, Training Loss: {train_loss}")

# Evaluate DistilBERT model
y_true_bert, y_pred_bert = evaluate_model(model, test_loader, device)
print("DistilBERT Model Evaluation:")
print(classification_report(y_true_bert, y_pred_bert))

# TF-IDF Vectorizer
vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = vectorizer.fit_transform(train_df['Chief Complaint'])
X_test_tfidf = vectorizer.transform(test_df['Chief Complaint'])

# Logistic Regression
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train_tfidf, train_df['label'])
y_pred_log_reg = log_reg.predict(X_test_tfidf)
print("Logistic Regression Evaluation:")
print(classification_report(test_df['label'], y_pred_log_reg))

# SVM
svm = SVC(kernel='linear')
svm.fit(X_train_tfidf, train_df['label'])
y_pred_svm = svm.predict(X_test_tfidf)
print("SVM Evaluation:")
print(classification_report(test_df['label'], y_pred_svm))

# Random Forest
rf = RandomForestClassifier(n_estimators=100)
rf.fit(X_train_tfidf, train_df['label'])
y_pred_rf = rf.predict(X_test_tfidf)
print("Random Forest Evaluation:")
print(classification_report(test_df['label'], y_pred_rf))

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, RobertaTokenizer, RobertaForSequenceClassification, AdamW

# Load and preprocess data
df = pd.read_csv(r"/content/GOUT-CC-2019-CORPUS-SYNTHETIC.tsv", delimiter='\t')

# Convert labels to numerical values
df['label'] = df['Predict'].astype('category').cat.codes

# Check unique labels
num_classes = df['label'].nunique()
print(f"Number of classes: {num_classes}")

# Split data
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# Define a custom dataset class
class TextDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_len):
        self.dataframe = dataframe
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, idx):
        row = self.dataframe.iloc[idx]
        text = row['Chief Complaint']
        label = row['label']

        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            return_tensors='pt',
            padding='max_length',
            truncation=True
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# Initialize tokenizer and model for DistilBERT
distilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
distilbert_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_classes)

# Initialize tokenizer and model for RoBERTa
roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
roberta_model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=num_classes)

# Create DataLoader for DistilBERT
distilbert_train_dataset = TextDataset(train_df, distilbert_tokenizer, max_len=128)
distilbert_train_loader = DataLoader(distilbert_train_dataset, batch_size=16, shuffle=True)

distilbert_test_dataset = TextDataset(test_df, distilbert_tokenizer, max_len=128)
distilbert_test_loader = DataLoader(distilbert_test_dataset, batch_size=16, shuffle=False)

# Create DataLoader for RoBERTa
roberta_train_dataset = TextDataset(train_df, roberta_tokenizer, max_len=128)
roberta_train_loader = DataLoader(roberta_train_dataset, batch_size=16, shuffle=True)

roberta_test_dataset = TextDataset(test_df, roberta_tokenizer, max_len=128)
roberta_test_loader = DataLoader(roberta_test_dataset, batch_size=16, shuffle=False)

# Set up optimizer for both models
distilbert_optimizer = AdamW(distilbert_model.parameters(), lr=1e-5)
roberta_optimizer = AdamW(roberta_model.parameters(), lr=1e-5)

# Define training function for BERT models
def train_model(model, data_loader, optimizer, device):
    model.train()
    total_loss = 0

    for batch in data_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    return total_loss / len(data_loader)

# Define evaluation function for BERT models
def evaluate_model(model, data_loader, device):
    model.eval()
    y_true = []
    y_pred = []

    with torch.no_grad():
        for batch in data_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            preds = torch.argmax(outputs.logits, dim=-1)

            y_true.extend(labels.cpu().numpy())
            y_pred.extend(preds.cpu().numpy())

    return y_true, y_pred

# Training and evaluation for DistilBERT
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
distilbert_model.to(device)

for epoch in range(4):
    train_loss = train_model(distilbert_model, distilbert_train_loader, distilbert_optimizer, device)
    print(f"DistilBERT - Epoch {epoch + 1}/4, Training Loss: {train_loss}")

# Evaluate DistilBERT model
y_true_bert, y_pred_bert = evaluate_model(distilbert_model, distilbert_test_loader, device)
print("DistilBERT Model Evaluation:")
print(classification_report(y_true_bert, y_pred_bert))

# Training and evaluation for RoBERTa
roberta_model.to(device)

for epoch in range(4):
    train_loss = train_model(roberta_model, roberta_train_loader, roberta_optimizer, device)
    print(f"RoBERTa - Epoch {epoch + 1}/4, Training Loss: {train_loss}")

# Evaluate RoBERTa model
y_true_roberta, y_pred_roberta = evaluate_model(roberta_model, roberta_test_loader, device)
print("RoBERTa Model Evaluation:")
print(classification_report(y_true_roberta, y_pred_roberta))

# TF-IDF Vectorizer
vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = vectorizer.fit_transform(train_df['Chief Complaint'])
X_test_tfidf = vectorizer.transform(test_df['Chief Complaint'])

# Logistic Regression
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train_tfidf, train_df['label'])
y_pred_log_reg = log_reg.predict(X_test_tfidf)
print("Logistic Regression Evaluation:")
print(classification_report(test_df['label'], y_pred_log_reg))

# SVM
svm = SVC(kernel='linear')
svm.fit(X_train_tfidf, train_df['label'])
y_pred_svm = svm.predict(X_test_tfidf)
print("SVM Evaluation:")
print(classification_report(test_df['label'], y_pred_svm))

# Random Forest
rf = RandomForestClassifier(n_estimators=100)
rf.fit(X_train_tfidf, train_df['label'])
y_pred_rf = rf.predict(X_test_tfidf)
print("Random Forest Evaluation:")
print(classification_report(test_df['label'], y_pred_rf))

import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# Load and preprocess data
df = pd.read_csv("/content/GOUT-CC-2019-CORPUS-SYNTHETIC.tsv", delimiter='\t')  # Replace with your actual data path

# Filter the dataset to include only 'Y' and 'N'
df = df[df['Predict'].isin(['Y', 'N'])]

# Convert 'Predict' column to numerical labels
df['label'] = df['Predict'].map({'Y': 1, 'N': 0})

# Split the dataset into training and testing sets
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# Define a custom dataset class
class TextDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_len):
        self.dataframe = dataframe
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, idx):
        row = self.dataframe.iloc[idx]
        text = row['Chief Complaint']
        label = row['label']

        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            return_tensors='pt',
            padding='max_length',
            truncation=True
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# Initialize tokenizer and model
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)

# Create DataLoader
train_dataset = TextDataset(train_df, tokenizer, max_len=128)
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)

test_dataset = TextDataset(test_df, tokenizer, max_len=128)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

# Set up optimizer
optimizer = AdamW(model.parameters(), lr=1e-5)

# Define training function
def train_model(model, data_loader, optimizer, device):
    model.train()
    total_loss = 0

    for batch in data_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    return total_loss / len(data_loader)

# Define evaluation function
def evaluate_model(model, data_loader, device):
    model.eval()
    y_true = []
    y_pred = []

    with torch.no_grad():
        for batch in data_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            preds = torch.argmax(outputs.logits, dim=-1)

            y_true.extend(labels.cpu().numpy())
            y_pred.extend(preds.cpu().numpy())

    return y_true, y_pred

# Training and evaluation
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

for epoch in range(4):
    train_loss = train_model(model, train_loader, optimizer, device)
    print(f"Epoch {epoch + 1}/{4}, Training Loss: {train_loss}")

# Evaluate the model
y_true, y_pred = evaluate_model(model, test_loader, device)

# Print classification report
print("Model Evaluation:")
print(classification_report(y_true, y_pred, target_names=['N', 'Y']))

import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, RobertaTokenizer, RobertaForSequenceClassification, AdamW
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# Load and preprocess data
df = pd.read_csv(r"/content/GOUT-CC-2019-CORPUS-SYNTHETIC.tsv", delimiter='\t')

# Convert labels to numerical values
df['label'] = df['Predict'].astype('category').cat.codes

# Check unique labels
num_classes = df['label'].nunique()
print(f"Number of classes: {num_classes}")

# Split data
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# Define a custom dataset class
class TextDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_len):
        self.dataframe = dataframe
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, idx):
        row = self.dataframe.iloc[idx]
        text = row['Chief Complaint']
        label = row['label']

        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            return_tensors='pt',
            padding='max_length',
            truncation=True
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# Initialize tokenizer and model for DistilBERT
distilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
distilbert_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_classes)

# Initialize tokenizer and model for RoBERTa
roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
roberta_model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=num_classes)

# Create DataLoader for DistilBERT
distilbert_train_dataset = TextDataset(train_df, distilbert_tokenizer, max_len=128)
distilbert_train_loader = DataLoader(distilbert_train_dataset, batch_size=16, shuffle=True)

distilbert_test_dataset = TextDataset(test_df, distilbert_tokenizer, max_len=128)
distilbert_test_loader = DataLoader(distilbert_test_dataset, batch_size=16, shuffle=False)

# Create DataLoader for RoBERTa
roberta_train_dataset = TextDataset(train_df, roberta_tokenizer, max_len=128)
roberta_train_loader = DataLoader(roberta_train_dataset, batch_size=16, shuffle=True)

roberta_test_dataset = TextDataset(test_df, roberta_tokenizer, max_len=128)
roberta_test_loader = DataLoader(roberta_test_dataset, batch_size=16, shuffle=False)

# Set up optimizer for both models
distilbert_optimizer = AdamW(distilbert_model.parameters(), lr=1e-5)
roberta_optimizer = AdamW(roberta_model.parameters(), lr=1e-5)

# Define training function for BERT models
def train_model(model, data_loader, optimizer, device):
    model.train()
    total_loss = 0

    for batch in data_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    return total_loss / len(data_loader)

# Define evaluation function for BERT models
def evaluate_model(model, data_loader, device):
    model.eval()
    y_true = []
    y_pred = []

    with torch.no_grad():
        for batch in data_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            preds = torch.argmax(outputs.logits, dim=-1)

            y_true.extend(labels.cpu().numpy())
            y_pred.extend(preds.cpu().numpy())

    return y_true, y_pred

# Training and evaluation for DistilBERT
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
distilbert_model.to(device)

for epoch in range(4):
    train_loss = train_model(distilbert_model, distilbert_train_loader, distilbert_optimizer, device)
    print(f"DistilBERT - Epoch {epoch + 1}/4, Training Loss: {train_loss}")

# Evaluate DistilBERT model
y_true_bert, y_pred_bert = evaluate_model(distilbert_model, distilbert_test_loader, device)
print("DistilBERT Model Evaluation:")
print(classification_report(y_true_bert, y_pred_bert))

# Training and evaluation for RoBERTa
roberta_model.to(device)

for epoch in range(4):
    train_loss = train_model(roberta_model, roberta_train_loader, roberta_optimizer, device)
    print(f"RoBERTa - Epoch {epoch + 1}/4, Training Loss: {train_loss}")

# Evaluate RoBERTa model
y_true_roberta, y_pred_roberta = evaluate_model(roberta_model, roberta_test_loader, device)
print("RoBERTa Model Evaluation:")
print(classification_report(y_true_roberta, y_pred_roberta))

# TF-IDF Vectorizer
vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = vectorizer.fit_transform(train_df['Chief Complaint'])
X_test_tfidf = vectorizer.transform(test_df['Chief Complaint'])

# Logistic Regression
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train_tfidf, train_df['label'])
y_pred_log_reg = log_reg.predict(X_test_tfidf)
print("Logistic Regression Evaluation:")
print(classification_report(test_df['label'], y_pred_log_reg))

# SVM
svm = SVC(kernel='linear')
svm.fit(X_train_tfidf, train_df['label'])
y_pred_svm = svm.predict(X_test_tfidf)
print("SVM Evaluation:")
print(classification_report(test_df['label'], y_pred_svm))

# Random Forest
rf = RandomForestClassifier(n_estimators=100)
rf.fit(X_train_tfidf, train_df['label'])
y_pred_rf = rf.predict(X_test_tfidf)
print("Random Forest Evaluation:")
print(classification_report(test_df['label'], y_pred_rf))

